# Python 网页爬虫与数据存储项目设计文档

## 1. 项目概述

本项目是一个基于 Python 的网页爬虫系统，主要用于从特定 API 获取 JSON 格式的数据，并将其存储到 MySQL 数据库中以便后续分析。项目遵循模块化设计原则，具有良好的可扩展性和可维护性。

### 1.1 项目目标

- 自动化获取目标 API 的 JSON 数据
- 将获取的数据结构化并存储至 MySQL 数据库
- 提供灵活的配置选项和命令行接口
- 实现完善的日志记录和错误处理机制

### 1.2 技术栈

- **编程语言**: Python 3
- **HTTP 请求**: requests
- **数据库**: MySQL
- **数据库连接**: mysql-connector-python
- **虚拟环境管理**: venv

## 2. 系统架构

项目采用简洁的模块化架构，主要分为以下几个核心组件：

```
+----------------+      +----------------+      +----------------+
|                |      |                |      |                |
|  配置模块      +----->+  爬虫模块      +----->+  数据库模块    |
|                |      |                |      |                |
+----------------+      +----------------+      +----------------+
                                |
                                v
                        +----------------+
                        |                |
                        |  工具函数模块  |
                        |                |
                        +----------------+
```

### 2.1 项目结构

```
project_root/
├── config/                     # 配置文件目录
│   ├── settings.py             # 爬虫设置（URL、请求头等）
│   └── db_config.py            # 数据库连接配置
├── src/                        # 源代码目录
│   ├── database.py             # 数据库操作
│   ├── scraper.py              # 爬虫逻辑
│   └── utils.py                # 工具函数
├── logs/                       # 日志目录（自动创建）
├── .cursor/docs                      # 项目文档
│   └── 项目设计文档.md         # 本文档
├── venv/                       # Python虚拟环境
├── main.py                     # 主程序入口
└── requirements.txt            # 项目依赖
```

## 3. 模块设计

### 3.1 配置模块 (`config/`)

#### 3.1.1 `settings.py`

该文件包含爬虫相关的配置信息，包括：

- 目标 URL 列表
- 默认 HTTP 请求头

这些配置项可以根据实际爬取需求进行调整。

#### 3.1.2 `db_config.py`

该文件存储 MySQL 数据库的连接信息，包括：

- 主机地址
- 用户名
- 密码
- 数据库名
- 端口号

**安全性注意事项**: 实际部署时，建议使用环境变量或.env 文件来存储敏感信息。

### 3.2 爬虫模块 (`src/scraper.py`)

爬虫模块负责从配置的 URL 获取 JSON 数据，并将其处理成适合存储的格式。主要功能包括：

1. **数据获取**: 使用`requests`库发送 HTTP 请求，获取响应内容。
2. **数据解析**: 将响应内容解析为 JSON 格式。
3. **数据处理**: 根据预定义的规则，提取并转换数据。
4. **错误处理**: 包含完善的异常处理机制，处理网络超时、服务器错误等异常情况。

主要函数:

- `fetch_data()`: 从指定 URL 获取 JSON 数据
- `process_json_data()`: 处理 JSON 数据，提取所需字段
- `scrape_all_targets()`: 爬取所有目标 URL 并存储数据

### 3.3 数据库模块 (`src/database.py`)

数据库模块负责处理与 MySQL 数据库的交互，包括连接管理、表创建和数据操作。主要功能包括：

1. **连接管理**: 创建、管理和关闭数据库连接。
2. **表结构定义**: 创建数据表（如果不存在）。
3. **数据操作**: 提供数据插入、查询等功能。

主要函数:

- `get_connection()`: 创建 MySQL 数据库连接
- `create_tables()`: 创建数据表（如果不存在）
- `insert_data()`: 插入单条数据
- `batch_insert()`: 批量插入数据
- `query_data()`: 执行查询并返回结果

### 3.4 工具函数模块 (`src/utils.py`)

工具函数模块提供各种辅助功能，如日志设置、文件操作和错误处理等。主要功能包括：

1. **日志管理**: 配置和管理日志系统。
2. **文件操作**: 提供 JSON 文件的读写功能。
3. **错误处理**: 包含处理 API 速率限制等特定情况的函数。

主要函数:

- `setup_logging()`: 设置详细的日志配置
- `save_to_json()`: 将数据保存为 JSON 文件
- `load_from_json()`: 从 JSON 文件加载数据
- `get_timestamp()`: 获取格式化的时间戳
- `handle_rate_limit()`: 处理 API 速率限制

### 3.5 主程序入口 (`main.py`)

主程序负责协调各个模块的工作，处理命令行参数，并执行相应的操作。主要功能包括：

1. **参数解析**: 使用`argparse`库解析命令行参数。
2. **日志配置**: 设置日志系统。
3. **流程控制**: 根据参数执行相应的操作（如只设置数据库、执行爬取等）。

命令行参数:

- `--log`: 指定日志文件路径
- `--backup`: 备份抓取的数据为 JSON 文件
- `--setup-db`: 仅设置数据库表结构，不爬取数据

## 4. 数据流

项目的数据流如下:

1. 从`config/settings.py`中读取目标 URL
2. 爬虫模块(`src/scraper.py`)发送 HTTP 请求获取数据
3. 解析并处理 JSON 响应数据
4. 将处理后的数据传递给数据库模块(`src/database.py`)
5. 数据库模块将数据存储到 MySQL 数据库

## 5. 扩展与优化计划

未来可能的扩展与优化方向包括：

1. **添加分析模块**: 创建`src/analyzer.py`模块，用于数据分析和可视化
2. **添加代理池**: 实现 IP 代理池，避免因频繁请求被目标网站封禁
3. **定时任务**: 实现爬取任务的定时执行功能
4. **Web 界面**: 添加简单的 Web 界面，用于监控爬取状态和浏览数据
5. **并发爬取**: 实现多线程或异步爬取，提高数据获取效率

## 6. 使用指南

### 6.1 安装依赖

```bash
# 创建并激活虚拟环境
python -m venv venv
source venv/bin/activate  # Linux/macOS
# 或
venv\Scripts\activate     # Windows

# 安装依赖
pip install -r requirements.txt
```

### 6.2 配置修改

1. 编辑`config/db_config.py`，设置 MySQL 数据库连接信息
2. 编辑`config/settings.py`，添加目标 URL
3. 根据实际数据结构，修改`src/database.py`中的表结构定义
4. 根据 API 返回的 JSON 格式，修改`src/scraper.py`中的数据处理逻辑

### 6.3 运行项目

```bash
# 仅创建数据库表
python main.py --setup-db

# 执行爬取任务
python main.py

# 执行爬取任务并保存日志到指定文件
python main.py --log ./my_scraper.log
```

## 7. 版本历史

### v1.0.0 (当前版本)

- 初始项目结构搭建
- 实现基础爬虫功能
- 实现 MySQL 数据存储
- 实现日志系统和错误处理
